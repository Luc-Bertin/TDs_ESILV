{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import github_command as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.push(file_to_transfer='TD7_Image_Captioning_CNN_RNN.ipynb',\n",
    "       message=\"model definition\",\n",
    "       repos=\"TDs_ESILV.git\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def timing(f):\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        print('func call took: {0:.2f} sec'.format(te-ts))\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image captionning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Network part\n",
    "#### Get the InceptionV3 model trained on imagenet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "model = InceptionV3(weights='imagenet')\n",
    "# Remove the last layer (output softmax layer) from the inception v3\n",
    "model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_proj_path=\"/Users/lucbertin/Downloads/demos/\"\n",
    "flickr_folder = \"flickr30k_images/\"\n",
    "captions_file = \"results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'image_name| comment_number| comment\\n1000092795.jpg| 0| Two young guys with shaggy hair look at their hands while hanging out in the yard .\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.check_output([\"head\", \"-n\", \"2\", folder_proj_path+flickr_folder+captions_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_name  comment_number  \\\n",
       "0  1000092795.jpg               0   \n",
       "1  1000092795.jpg               1   \n",
       "2  1000092795.jpg               2   \n",
       "3  1000092795.jpg               3   \n",
       "4  1000092795.jpg               4   \n",
       "\n",
       "                                             comment  \n",
       "0   Two young guys with shaggy hair look at their...  \n",
       "1   Two young , White males are outside near many...  \n",
       "2   Two men in green shirts are standing in a yard .  \n",
       "3       A man in a blue shirt standing in a garden .  \n",
       "4            Two friends enjoy time spent together .  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "## Open descriptions dataset and append corresponding images\n",
    "df = pd.read_csv(folder_proj_path+flickr_folder+captions_file, sep='|')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158915, 3)\n",
      "Index(['image_name', 'comment_number', 'comment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Appending startseq and endseq to each comment\n",
    "df['comment2'] = (\"startseq \"  +  df.comment\n",
    "                                        .str.lower()\n",
    "                                        .str.replace(r\"[^a-z0-9 ]\", \"\")\n",
    "                                        .str.split().str.join(\" \") + \" endseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19999</td>\n",
       "      <td>2199200615.jpg</td>\n",
       "      <td>4   A dog runs across the grass .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_name                      comment_number comment comment2\n",
       "19999  2199200615.jpg   4   A dog runs across the grass .     NaN      NaN"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['comment2'].apply(lambda x: type(x)==float)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lol\n",
    "df.drop(19999, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5463"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Just take words occuring at least 10 times\n",
    "from collections import Counter\n",
    "all_words = [item for sublist in df.comment2.str.split(' ').tolist() for item in sublist]\n",
    "more_than_10_occurences = {k:val for k,val in Counter(all_words).items() if val>=10}\n",
    "\n",
    "#more_than_10_occurences\n",
    "df['comment2'] = df['comment2'].str.split(\" \").apply(lambda val: [x for x in val if more_than_10_occurences.get(x) is not None])\n",
    "len(more_than_10_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE=(299,299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(image, model_new):\n",
    "    \"\"\" Function to encode a given image into a vector of size (2048, ) using inceptionV3 \"\"\"\n",
    "    from keras.applications.inception_v3 import preprocess_input\n",
    "    import numpy as np\n",
    "    image = np.array(image) # transform img to array\n",
    "    image = np.expand_dims(image, axis=0) # add one dimension for batch (keras needs it)\n",
    "    image = preprocess_input(image) # preprocess the image for inceptionV3\n",
    "    fea_vec = model_new.predict(image) # The model beign trained already, get the encoding vector for the image after a forward pass\n",
    "    fea_vec = np.reshape(fea_vec, -1) # reshape from (1, 2048) to (2048, )\n",
    "    return fea_vec\n",
    "\n",
    "def create_dictionnaries_for_string_convertion(vocab):\n",
    "    \"\"\" Create an index to word dictionnary and a word to index one \"\"\"\n",
    "    ixtoword, wordtoix = {}, {}\n",
    "    ix = 1\n",
    "    for w in vocab:\n",
    "        wordtoix[w] = ix\n",
    "        ixtoword[ix] = w\n",
    "        ix += 1\n",
    "    return ixtoword, wordtoix\n",
    "\n",
    "ixtoword, wordtoix = create_dictionnaries_for_string_convertion(vocab=more_than_10_occurences)\n",
    "maximum_length_caption_on_all_dataset = max(df.comment2.apply(len)) # max caption length ( for homogeneity of input vectors )\n",
    "maximum_length_caption_on_all_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'startseq a man wearing a helmet red pants with white stripes going down the sides and a white and red shirt is on a small bicycle using only his hands while his legs are up in the air while another man wearing a light blue shirt with dark blue trim and black pants with red stripes going up the sides is standing nearby gesturing toward the first man and holding a small of one of the seven endseq'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(df[df[\"comment2\"].apply(lambda x: len(x)==78)].comment2[16050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"{1: 'startseq', 2: 'two', 3: 'young', 4: 'guys', 5: 'with', 6: 'shaggy', 7: 'hair', 8: 'look', 9: 'a\",\n",
       " \"{'startseq': 1, 'two': 2, 'young': 3, 'guys': 4, 'with': 5, 'shaggy': 6, 'hair': 7, 'look': 8, 'at':\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ixtoword)[:100], str(wordtoix)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoding sequence and padding (for creation of inputs) #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding__padding_inputs_seq(sequence, vocab, shift, max_length):\n",
    "    import numpy as np\n",
    "    encoding = list(map(vocab.get, sequence[:shift+1]))\n",
    "    encoding += [0]*(max_length-len(encoding))\n",
    "    return np.array(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"the\", \"rabbit\", \"is\", \"in\", \"the\",\"kitchen\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([16,  0,  0,  0,  0,  0]),\n",
       " array([  16, 4380,    0,    0,    0,    0]),\n",
       " array([  16, 4380,   68,    0,    0,    0]),\n",
       " array([  16, 4380,   68,   15,    0,    0]),\n",
       " array([  16, 4380,   68,   15,   16,    0]),\n",
       " array([  16, 4380,   68,   15,   16,  105])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding__padding_inputs_seq(sequence=a, shift=i, vocab=wordtoix, max_length=6) for i in range(len(a))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding based on vocab encoding for outputs (result in softmax) #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_outputs_from_seq(sequence, shift, vocab):\n",
    "    import numpy as np\n",
    "    encoding = vocab.get(sequence[shift])\n",
    "    return np.eye(len(vocab)+1)[encoding, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding_outputs_from_seq(sequence=a, shift=i, vocab=wordtoix) for i in range(len(a))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a batch of images then the sequence of Xt inputs with respective targets Yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def load_batch_of_images(df, batch_size, model_for_encoding, folder_imgs_path, TARGET_SIZE=TARGET_SIZE):\n",
    "    \"\"\" This function will only load batch_size pictures at a time (for computations)\"\"\"\n",
    "    from PIL import Image as Im\n",
    "    import pandas as pd, numpy as np\n",
    "    \n",
    "    # iterate for ever (check Keras documentation)\n",
    "    while 1:\n",
    "        print(\"shuffling...\")\n",
    "        ## shuffling\n",
    "        df = df.sample(n=len(df))\n",
    "        ## One epoch = One loop\n",
    "        print(\"starting epochs\")\n",
    "        for batch_i in range(1, len(df)//batch_size):\n",
    "            print(\"batch number : {} |Â batch_size : {}\".format(batch_i, batch_size))\n",
    "            \n",
    "            ### ======== Images encoding part ======== ####\n",
    "            # take a sample from the main dataset\n",
    "            df_sub = df.iloc[:batch_i*batch_size]#.copy()#reset_index(drop=True)\n",
    "            df_sub_images, df_sub_comments = df_sub[['image_name']].copy(), df_sub[[\"image_name\", \"comment2\"]].copy()\n",
    "            display(df_sub)\n",
    "            \n",
    "            print(\"opening corresponding images in {}\".format(folder_imgs_path))\n",
    "            # open corresponding images in new column\n",
    "            df_sub_images['image'] = df_sub_images.image_name.apply( lambda x: Im.open(folder_imgs_path+x).resize(TARGET_SIZE))\n",
    "            \n",
    "            print(\"encoding images using Inceptionv3 model frozen layers\")\n",
    "            # transform to array, preprocess and encode images\n",
    "            df_sub_images['image'] = df_sub_images.image.apply(lambda x: encode(x, model_for_encoding))\n",
    "            \n",
    "            \n",
    "            ### ======== Word sequence convertion to index then embedding part ======== ####\n",
    "            print(\"starting sequence convertion to index based on entire dataset vocabulary\")\n",
    "            df_sub_comments['input_sequences']  = df_sub_comments.comment2.apply(lambda x: \n",
    "                        [encoding__padding_inputs_seq(sequence=x, shift=i, vocab=wordtoix, max_length=maximum_length_caption_on_all_dataset) for i in range(len(x))])\n",
    "            df_sub_comments['output_sequences'] = df_sub_comments.comment2.apply(lambda x:\n",
    "                        [encoding_outputs_from_seq(sequence=x, shift=i, vocab=wordtoix) for i in range(len(x))])\n",
    "            df_sub_comments = df_sub_comments.apply(lambda x: x.apply(pd.Series).stack()).reset_index(drop=True).ffill()\n",
    "            \n",
    "            print(\"ðŸ”¥inputs ready.\")\n",
    "            #display(df_sub_images)\n",
    "            #display(df_sub_comments)\n",
    "            \n",
    "            df_sub = df_sub_images.merge(df_sub_comments, on=[\"image_name\"])\n",
    "            #display(df_sub)\n",
    "            ## problem in input1 shape: not sum of both shapes\n",
    "            # [[input1, input2],  output]\n",
    "            return [ [np.stack(df_sub.image), np.stack(df_sub.input_sequences)], np.stack(df_sub.output_sequences) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling...\n",
      "starting epochs\n",
      "batch number : 1 |Â batch_size : 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>77613</td>\n",
       "      <td>3585117340.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A pile of dogs sleep on a blue dog bed .</td>\n",
       "      <td>[startseq, a, pile, of, dogs, sleep, on, a, bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155167</td>\n",
       "      <td>8045061561.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>A young blond woman in a blue jersey holds he...</td>\n",
       "      <td>[startseq, a, young, blond, woman, in, a, blue...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_name comment_number  \\\n",
       "77613   3585117340.jpg              3   \n",
       "155167  8045061561.jpg              2   \n",
       "\n",
       "                                                  comment  \\\n",
       "77613            A pile of dogs sleep on a blue dog bed .   \n",
       "155167   A young blond woman in a blue jersey holds he...   \n",
       "\n",
       "                                                 comment2  \n",
       "77613   [startseq, a, pile, of, dogs, sleep, on, a, bl...  \n",
       "155167  [startseq, a, young, blond, woman, in, a, blue...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening corresponding images in /Users/lucbertin/Downloads/demos/flickr30k_images/\n",
      "encoding images using Inceptionv3 model frozen layers\n",
      "starting sequence convertion to index based on entire dataset vocabulary\n",
      "ðŸ”¥inputs ready.\n",
      "func call took: 0.70 sec\n"
     ]
    }
   ],
   "source": [
    "[[ input1, input2], output] = load_batch_of_images(df, batch_size=2, \n",
    "                                                 model_for_encoding=model_new, \n",
    "                                                 folder_imgs_path=folder_proj_path+flickr_folder)\n",
    "#sub.loc[0, 'image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 78)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 5464)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2048)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "### gives a vector representation of words converted into numerical indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#@email_sender(recipient_emails=[\"<your_email@address.com>\", \"<your_second_email@address.com>\"], sender_email=\"<grandma's_email@gmail.com>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file=\"glove/glove.6B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'the -0.071549 0.093459 0.023738 -0.090339 0.056123 0.32547 -0.39796 -0.092139 0.061181 -0.1895 0.13061 0.14349 0.011479 0.38158 0.5403 -0.14088 0.24315 0.23036 -0.55339 0.048154 0.45662 3.2338 0.020199 0.049019 -0.014132 0.076017 -0.11527 0.2006 -0.077657 0.24328 0.16368 -0.34118 -0.06607 0.10152 0.038232 -0.17668 -0.88153 -0.33895 -0.035481 -0.55095 -0.016899 -0.43982 0.039004 0.40447 -0.2588 0.64594 0.26641 0.28009 -0.024625 0.63302 -0.317 0.10271 0.30886 0.097792 -0.38227 0.086552 0.047075 0.23511 -0.32127 -0.28538 0.1667 -0.0049707 -0.62714 -0.24904 0.29713 0.14379 -0.12325 -0.058178 -0.001029 -0.082126 0.36935 -0.00058442 0.34286 0.28426 -0.068599 0.65747 -0.029087 0.16184 0.073672 -0.30343 0.095733 -0.5286 -0.22898 0.064079 0.015218 0.34921 -0.4396 -0.43983 0.77515 -0.87767 -0.087504 0.39598 0.62362 -0.26211 -0.30539 -0.022964 0.30567 0.06766 0.15383 -0.11211 -0.09154 0.082562 0.16897 -0.032952 -0.28775 -0.2232 -0.090426 1.2407 -0.18244 -0.0075219 -0.041388 -0.011083 0.078186 0.38511 0.23334 0.14414 -0.0009107 -0.26388 -0.20481 0.10099 0.14076 0.28834 -0.045429 0.37247 0.13645 -0.67457 0.22786 0.12599 0.029091 0.030428 -0.13028 0.19408 0.49014 -0.39121 -0.075952 0.074731 0.18902 -0.16922 -0.26019 -0.039771 -0.24153 0.10875 0.30434 0.036009 1.4264 0.12759 -0.073811 -0.20418 0.0080016 0.15381 0.20223 0.28274 0.096206 -0.33634 0.50983 0.32625 -0.26535 0.374 -0.30388 -0.40033 -0.04291 -0.067897 -0.29332 0.10978 -0.045365 0.23222 -0.31134 -0.28983 -0.66687 0.53097 0.19461 0.3667 0.26185 -0.65187 0.10266 0.11363 -0.12953 -0.68246 -0.18751 0.1476 1.0765 -0.22908 -0.0093435 -0.20651 -0.35225 -0.2672 -0.0034307 0.25906 0.21759 0.66158 0.1218 0.19957 -0.20303 0.34474 -0.24328 0.13139 -0.0088767 0.33617 0.030591 0.25577\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.check_output([\"head\", \"-n\", \"1\", folder_proj_path+glove_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the whole embedding into memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embeddings_index = {} # empty dictionary\n",
    "with open(folder_proj_path+glove_file, encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Loaded {} word vectors.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform integer vector representation to dense one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* embeddings_index associate a **word** to a **vector representation**\n",
    "* wordtoix associate a **word** to a **integer number**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 200-dim dense vector for each of the 5464 words in out vocabulary (word_to_idx)\n",
    "embedding_matrix = np.zeros((len(wordtoix)+1, 200)) # 200: embedding dim: the Dense representation of the word with '200 like features'\n",
    "for word, i in wordtoix.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5464, 200)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To each 5464 word is **associated a vector** \n",
    "* It's a **stack of vectors and the index i of the matrix is associated to the index of the word itself**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_length_caption_on_all_dataset\n",
    "vocab_size = len(wordtoix) + 1\n",
    "embedding_dim = 200 # dense words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, LSTM, Input, Embedding, add\n",
    "from keras.models import Model\n",
    "\n",
    "# image feature extractor model (as 2048 vector of features)\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1     = Dropout(0.5)(inputs1)\n",
    "fe2     = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# partial caption sequence model (as max size of sequences (padding: 78 values in list))\n",
    "inputs2 = Input(shape=(maximum_length_caption_on_all_dataset,)) \n",
    "se1     = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False)(inputs2)\n",
    "se2     = Dropout(0.5)(se1)\n",
    "se3     = LSTM(256)(se2)\n",
    "\n",
    "# decoder (feed forward) model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs  = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# merge the two input models\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 78)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 78, 200)      1092800     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 78, 200)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          467968      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 5464)         1404248     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,555,352\n",
      "Trainable params: 2,462,552\n",
      "Non-trainable params: 1,092,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "steps = len(df)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4966"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffling\n",
    "df = df.sample(n=len(df))\n",
    "## splitting\n",
    "train_size = 0.999\n",
    "df_train, df_test = df.iloc[:round(train_size*len(df))], df.iloc[round(train_size*len(df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((158914, 4), (158755, 4), (159, 4))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling...\n",
      "starting epochs\n",
      "batch number : 1 |Â batch_size : 159\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>1057210460.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>A man puts his shirt on near an elevator .</td>\n",
       "      <td>[startseq, a, man, puts, his, shirt, on, near,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59292</td>\n",
       "      <td>3182570190.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>People walk by a large advertisement of parti...</td>\n",
       "      <td>[startseq, people, walk, by, a, large, adverti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60973</td>\n",
       "      <td>3216901052.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A child in winter clothes is walking along th...</td>\n",
       "      <td>[startseq, a, child, in, winter, clothes, is, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143743</td>\n",
       "      <td>6458049647.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>guy riding his bike coming out of the forest .</td>\n",
       "      <td>[startseq, guy, riding, his, bike, coming, out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118941</td>\n",
       "      <td>4814419731.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Two children are climbing into a SpongeBob Sq...</td>\n",
       "      <td>[startseq, two, children, are, climbing, into,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28228</td>\n",
       "      <td>241347204.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A football player in red and white is holding...</td>\n",
       "      <td>[startseq, a, football, player, in, red, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121548</td>\n",
       "      <td>4854738791.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Two men walking their canoe through the woods .</td>\n",
       "      <td>[startseq, two, men, walking, their, canoe, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130732</td>\n",
       "      <td>50835360.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Some men are mowing hay in a field .</td>\n",
       "      <td>[startseq, some, men, are, mowing, hay, in, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70038</td>\n",
       "      <td>3422394336.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A beaver on the shore of a stream .</td>\n",
       "      <td>[startseq, a, on, the, shore, of, a, stream, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32933</td>\n",
       "      <td>2525871909.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>five children playing in front of a green pit...</td>\n",
       "      <td>[startseq, five, children, playing, in, front,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_name comment_number  \\\n",
       "879     1057210460.jpg              4   \n",
       "59292   3182570190.jpg              2   \n",
       "60973   3216901052.jpg              3   \n",
       "143743  6458049647.jpg              3   \n",
       "118941  4814419731.jpg              1   \n",
       "...                ...            ...   \n",
       "28228    241347204.jpg              3   \n",
       "121548  4854738791.jpg              3   \n",
       "130732    50835360.jpg              2   \n",
       "70038   3422394336.jpg              3   \n",
       "32933   2525871909.jpg              3   \n",
       "\n",
       "                                                  comment  \\\n",
       "879            A man puts his shirt on near an elevator .   \n",
       "59292    People walk by a large advertisement of parti...   \n",
       "60973    A child in winter clothes is walking along th...   \n",
       "143743     guy riding his bike coming out of the forest .   \n",
       "118941   Two children are climbing into a SpongeBob Sq...   \n",
       "...                                                   ...   \n",
       "28228    A football player in red and white is holding...   \n",
       "121548    Two men walking their canoe through the woods .   \n",
       "130732               Some men are mowing hay in a field .   \n",
       "70038                 A beaver on the shore of a stream .   \n",
       "32933    five children playing in front of a green pit...   \n",
       "\n",
       "                                                 comment2  \n",
       "879     [startseq, a, man, puts, his, shirt, on, near,...  \n",
       "59292   [startseq, people, walk, by, a, large, adverti...  \n",
       "60973   [startseq, a, child, in, winter, clothes, is, ...  \n",
       "143743  [startseq, guy, riding, his, bike, coming, out...  \n",
       "118941  [startseq, two, children, are, climbing, into,...  \n",
       "...                                                   ...  \n",
       "28228   [startseq, a, football, player, in, red, and, ...  \n",
       "121548  [startseq, two, men, walking, their, canoe, th...  \n",
       "130732  [startseq, some, men, are, mowing, hay, in, a,...  \n",
       "70038   [startseq, a, on, the, shore, of, a, stream, e...  \n",
       "32933   [startseq, five, children, playing, in, front,...  \n",
       "\n",
       "[159 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening corresponding images in /Users/lucbertin/Downloads/demos/flickr30k_images/\n",
      "encoding images using Inceptionv3 model frozen layers\n",
      "starting sequence convertion to index based on entire dataset vocabulary\n",
      "ðŸ”¥inputs ready.\n",
      "func call took: 58.68 sec\n"
     ]
    }
   ],
   "source": [
    "[[ input1_val, input2_val], output_val] = load_batch_of_images(df, batch_size=len(df_test), \n",
    "                                                 model_for_encoding=model_new, \n",
    "                                                 folder_imgs_path=folder_proj_path+flickr_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "159 photos and input sequences processed in 58 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ input1, input2], output] = load_batch_of_images(df, batch_size=2, \n",
    "                                                 model_for_encoding=model_new, \n",
    "                                                 folder_imgs_path=folder_proj_path+flickr_folder)\n",
    "#sub.loc[0, 'image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator=load_batch_of_images(\n",
    "                            df, batch_size=2, \n",
    "                            model_for_encoding=model_new, \n",
    "                            folder_imgs_path=folder_proj_path+flickr_folder), \n",
    "                    steps_per_epoch=steps,\n",
    "                    epochs=epochs, validation_data = ([ input1_val, input2_val], output_val),\n",
    "                    verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
